{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da310e01",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tuple, List\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d3100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = r\"C:\\Users\\sudeepta\\Desktop\\DR_project\\augmented_resized_V2\\train\"\n",
    "VAL_DIR   = r\"C:\\Users\\sudeepta\\Desktop\\DR_project\\augmented_resized_V2\\val\"\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "PROJ_DIM = 256\n",
    "NUM_CLASSES = 5\n",
    "HEAD_EPOCHS = 20\n",
    "PARTIAL_EPOCHS = 0\n",
    "FINE_EPOCHS = 30\n",
    "LR = 1e-3\n",
    "DEVICE_STR = 'cuda'  # use 'cpu' to force CPU\n",
    "SAVE_PATH = 'hybrid_checkpoint.pth'\n",
    "OUT_DIR = 'outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b14f99eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mConvBlock\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_ch, out_ch, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel=3, stride=1, padding=1, groups=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel, stride, padding, groups=groups, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "class VisionNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_features=PROJ_DIM):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            ConvBlock(in_channels, 32, kernel=3, stride=2),\n",
    "            ConvBlock(32, 32),\n",
    "            ConvBlock(32, 64, stride=2),\n",
    "        )\n",
    "        self.stage1 = nn.Sequential(\n",
    "            ConvBlock(64, 128),\n",
    "            ConvBlock(128, 128),\n",
    "        )\n",
    "        self.stage2 = nn.Sequential(\n",
    "            ConvBlock(128, 256, stride=2),\n",
    "            ConvBlock(256, 256),\n",
    "        )\n",
    "        self.stage3 = nn.Sequential(\n",
    "            ConvBlock(256, 512, stride=2),\n",
    "            ConvBlock(512, 512),\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self._out_dim = num_features\n",
    "        self.project = nn.Linear(512, self._out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.pool(x).flatten(1)\n",
    "        x = self.project(x)\n",
    "        return x\n",
    "\n",
    "    def feature_dim(self):\n",
    "        return self._out_dim\n",
    "\n",
    "class VisionMamba(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_features=PROJ_DIM):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            ConvBlock(in_channels, 24, stride=2),\n",
    "            ConvBlock(24, 24),\n",
    "        )\n",
    "        self.depthwise = nn.Sequential(\n",
    "            ConvBlock(24, 48, groups=24, stride=2),\n",
    "            ConvBlock(48, 48, groups=48),\n",
    "            ConvBlock(48, 96, stride=2),\n",
    "            ConvBlock(96, 96),\n",
    "        )\n",
    "        self.spp = nn.Sequential(\n",
    "            ConvBlock(96, 192, kernel=1, padding=0),\n",
    "            ConvBlock(192, 192),\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self._out_dim = num_features\n",
    "        self.project = nn.Linear(192, self._out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.depthwise(x)\n",
    "        x = self.spp(x)\n",
    "        x = self.pool(x).flatten(1)\n",
    "        x = self.project(x)\n",
    "        return x\n",
    "\n",
    "    def feature_dim(self):\n",
    "        return self._out_dim\n",
    "\n",
    "class CondenseBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel=3, stride=1, groups=4):\n",
    "        super().__init__()\n",
    "        self.pw = nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=False, groups=groups)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.act1 = nn.ReLU(inplace=True)\n",
    "        self.dw = nn.Conv2d(out_ch, out_ch, kernel_size=kernel, padding=kernel//2, stride=stride, groups=out_ch, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        self.act2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.bn1(self.pw(x)))\n",
    "        x = self.act2(self.bn2(self.dw(x)))\n",
    "        return x\n",
    "\n",
    "class CondenseNetSimple(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_features=PROJ_DIM):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            ConvBlock(in_channels, 32, stride=2),\n",
    "            CondenseBlock(32, 64, groups=4),\n",
    "            CondenseBlock(64, 128, groups=4),\n",
    "            CondenseBlock(128, 256, groups=8),\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self._out_dim = num_features\n",
    "        self.project = nn.Linear(256, self._out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.pool(x).flatten(1)\n",
    "        x = self.project(x)\n",
    "        return x\n",
    "\n",
    "    def feature_dim(self):\n",
    "        return self._out_dim\n",
    "\n",
    "class GatedProjection(nn.Module):\n",
    "    def __init__(self, in_dim, proj_dim=PROJ_DIM):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, proj_dim)\n",
    "        self.bn = nn.BatchNorm1d(proj_dim)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(proj_dim, max(8, proj_dim//8)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(max(8, proj_dim//8), 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.bn(self.fc(x)))\n",
    "        g = self.gate(x)\n",
    "        return x * g\n",
    "\n",
    "class HybridFusionModel(nn.Module):\n",
    "    def __init__(self, backboneA, backboneB, backboneC, proj_dim=PROJ_DIM, num_classes=NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        self.backA = backboneA\n",
    "        self.backB = backboneB\n",
    "        self.backC = backboneC\n",
    "        self.projA = GatedProjection(self.backA.feature_dim(), proj_dim)\n",
    "        self.projB = GatedProjection(self.backB.feature_dim(), proj_dim)\n",
    "        self.projC = GatedProjection(self.backC.feature_dim(), proj_dim)\n",
    "        fused_dim = proj_dim * 3\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        fA = self.backA(x)\n",
    "        fB = self.backB(x)\n",
    "        fC = self.backC(x)\n",
    "        pA = self.projA(fA)\n",
    "        pB = self.projB(fB)\n",
    "        pC = self.projC(fC)\n",
    "        fused = torch.cat([pA, pB, pC], dim=1)\n",
    "        out = self.head(fused)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd9a9d68",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IMAGE_SIZE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dataloaders_from_dirs\u001b[39m(train_dir: \u001b[38;5;28mstr\u001b[39m, val_dir: \u001b[38;5;28mstr\u001b[39m, image_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mIMAGE_SIZE\u001b[49m, batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m BATCH_SIZE, num_workers: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[DataLoader, DataLoader]:\n\u001b[0;32m      2\u001b[0m     train_tf \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      3\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mRandomResizedCrop(image_size),\n\u001b[0;32m      4\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mRandomHorizontalFlip(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m,\u001b[38;5;241m0.456\u001b[39m,\u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m,\u001b[38;5;241m0.224\u001b[39m,\u001b[38;5;241m0.225\u001b[39m])\n\u001b[0;32m      8\u001b[0m     ])\n\u001b[0;32m      9\u001b[0m     val_tf \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     10\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;28mint\u001b[39m(image_size\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1.14\u001b[39m)),\n\u001b[0;32m     11\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mCenterCrop(image_size),\n\u001b[0;32m     12\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     13\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m,\u001b[38;5;241m0.456\u001b[39m,\u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m,\u001b[38;5;241m0.224\u001b[39m,\u001b[38;5;241m0.225\u001b[39m])\n\u001b[0;32m     14\u001b[0m     ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'IMAGE_SIZE' is not defined"
     ]
    }
   ],
   "source": [
    "def get_dataloaders_from_dirs(train_dir: str, val_dir: str, image_size: int = IMAGE_SIZE, batch_size: int = BATCH_SIZE, num_workers: int = 4) -> Tuple[DataLoader, DataLoader]:\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(image_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(0.2, 0.2, 0.2, 0.02),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize(int(image_size*1.14)),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    train_ds = datasets.ImageFolder(train_dir, transform=train_tf)\n",
    "    val_ds = datasets.ImageFolder(val_dir, transform=val_tf)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# -------------------- Training utilities ------------------------------\n",
    "def save_checkpoint(state, fname=SAVE_PATH):\n",
    "    torch.save(state, fname)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_meter = 0.0\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(imgs)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss_meter += loss.item() * imgs.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += imgs.size(0)\n",
    "    return loss_meter / total, correct / total\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        total += imgs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "    avg_loss = running_loss / total\n",
    "    acc = correct / total\n",
    "    print(f\"Epoch {epoch} Train Loss: {avg_loss:.4f} Acc: {acc:.4f}\")\n",
    "    return avg_loss, acc\n",
    "\n",
    "# -------------------- Plotting & confusion ----------------------------\n",
    "def plot_metrics(train_losses: List[float], val_losses: List[float], train_accs: List[float], val_accs: List[float], out_dir: str):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    epochs = range(1, len(train_losses)+1)\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_losses, linestyle='-', marker='o')\n",
    "    plt.plot(epochs, val_losses, linestyle='-', marker='o')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Training & Validation Loss')\n",
    "    plt.legend(['train','val']); plt.grid(True); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, 'loss_curve.png')); plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_accs, linestyle='-', marker='o')\n",
    "    plt.plot(epochs, val_accs, linestyle='-', marker='o')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Training & Validation Accuracy')\n",
    "    plt.legend(['train','val']); plt.grid(True); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, 'acc_curve.png')); plt.close()\n",
    "    print('Saved plots to', out_dir)\n",
    "\n",
    "def compute_and_save_confusion(model, loader, device, class_names: List[str], out_dir: str):\n",
    "    model.eval()\n",
    "    preds_all = []\n",
    "    labels_all = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            logits = model(imgs)\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            preds_all.extend(preds.tolist())\n",
    "            labels_all.extend(labels.numpy().tolist())\n",
    "    cm = confusion_matrix(labels_all, preds_all)\n",
    "    print('Confusion matrix:\\n', cm)\n",
    "    print('Classification report:\\n', classification_report(labels_all, preds_all, target_names=class_names))\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title('Confusion matrix'); plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45); plt.yticks(tick_marks, class_names)\n",
    "    plt.ylabel('True label'); plt.xlabel('Predicted label'); plt.tight_layout()\n",
    "    cm_path = os.path.join(out_dir, 'confusion_matrix.png')\n",
    "    plt.savefig(cm_path); plt.close()\n",
    "    print('Saved confusion matrix to', cm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9effdae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 85\u001b[0m\n\u001b[0;32m     82\u001b[0m     compute_and_save_confusion(model, val_loader, device, class_names, OUT_DIR)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m----> 2\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(DEVICE_STR \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing device:\u001b[39m\u001b[38;5;124m'\u001b[39m, device)\n\u001b[0;32m      5\u001b[0m     visionnet \u001b[38;5;241m=\u001b[39m VisionNet(num_features\u001b[38;5;241m=\u001b[39mPROJ_DIM)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    device = torch.device(DEVICE_STR if torch.cuda.is_available() else 'cpu')\n",
    "    print('Using device:', device)\n",
    "\n",
    "    visionnet = VisionNet(num_features=PROJ_DIM)\n",
    "    visionmamba = VisionMamba(num_features=PROJ_DIM)\n",
    "    condensenet = CondenseNetSimple(num_features=PROJ_DIM)\n",
    "\n",
    "    model = HybridFusionModel(visionnet, visionmamba, condensenet, proj_dim=PROJ_DIM, num_classes=NUM_CLASSES)\n",
    "    model.to(device)\n",
    "\n",
    "    print(\"Train dir:\", TRAIN_DIR)\n",
    "    print(\"Val dir:  \", VAL_DIR)\n",
    "\n",
    "    train_loader, val_loader = get_dataloaders_from_dirs(TRAIN_DIR, VAL_DIR, IMAGE_SIZE, BATCH_SIZE)\n",
    "    class_names = sorted([d for d in os.listdir(TRAIN_DIR) if os.path.isdir(os.path.join(TRAIN_DIR, d))])\n",
    "\n",
    "    train_losses = []; val_losses = []; train_accs = []; val_accs = []\n",
    "\n",
    "    # Stage 1: freeze backbones, train head-only\n",
    "    for param in model.backA.parameters(): param.requires_grad = False\n",
    "    for param in model.backB.parameters(): param.requires_grad = False\n",
    "    for param in model.backC.parameters(): param.requires_grad = False\n",
    "    for param in model.projA.parameters(): param.requires_grad = True\n",
    "    for param in model.projB.parameters(): param.requires_grad = True\n",
    "    for param in model.projC.parameters(): param.requires_grad = True\n",
    "    for param in model.head.parameters(): param.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=1e-4)\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    print(f\"Head-only training for {HEAD_EPOCHS} epochs...\")\n",
    "    for epoch in range(1, HEAD_EPOCHS + 1):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, device, epoch)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, device)\n",
    "        train_losses.append(tr_loss); train_accs.append(tr_acc)\n",
    "        val_losses.append(val_loss); val_accs.append(val_acc)\n",
    "        print(f\"Validation after head epoch {epoch}: loss {val_loss:.4f}, acc {val_acc:.4f}\")\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            save_checkpoint({'epoch': epoch, 'model_state': model.state_dict(), 'acc': val_acc}, SAVE_PATH)\n",
    "\n",
    "    # Stage 2: partial unfreeze if requested\n",
    "    if PARTIAL_EPOCHS > 0:\n",
    "        print(f\"Partial unfreeze for {PARTIAL_EPOCHS} epochs...\")\n",
    "        for param in model.backA.stage3.parameters(): param.requires_grad = True\n",
    "        for param in model.backB.spp.parameters(): param.requires_grad = True\n",
    "        for param in model.backC.stem[-1].parameters(): param.requires_grad = True\n",
    "\n",
    "        optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR * 0.1, weight_decay=1e-4)\n",
    "        for epoch in range(HEAD_EPOCHS + 1, HEAD_EPOCHS + PARTIAL_EPOCHS + 1):\n",
    "            tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, device, epoch)\n",
    "            val_loss, val_acc = evaluate(model, val_loader, device)\n",
    "            train_losses.append(tr_loss); train_accs.append(tr_acc)\n",
    "            val_losses.append(val_loss); val_accs.append(val_acc)\n",
    "            print(f\"Validation after partial epoch {epoch}: loss {val_loss:.4f}, acc {val_acc:.4f}\")\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                save_checkpoint({'epoch': epoch, 'model_state': model.state_dict(), 'acc': val_acc}, SAVE_PATH)\n",
    "\n",
    "    # Stage 3: full fine-tune\n",
    "    print(f\"Full fine-tune for {FINE_EPOCHS} epochs...\")\n",
    "    for param in model.parameters(): param.requires_grad = True\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR * 0.01, weight_decay=1e-5)\n",
    "\n",
    "    start_epoch = HEAD_EPOCHS + PARTIAL_EPOCHS + 1\n",
    "    for i in range(FINE_EPOCHS):\n",
    "        epoch = start_epoch + i\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, device, epoch)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, device)\n",
    "        train_losses.append(tr_loss); train_accs.append(tr_acc)\n",
    "        val_losses.append(val_loss); val_accs.append(val_acc)\n",
    "        print(f\"Validation after fine epoch {epoch}: loss {val_loss:.4f}, acc {val_acc:.4f}\")\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            save_checkpoint({'epoch': epoch, 'model_state': model.state_dict(), 'acc': val_acc}, SAVE_PATH)\n",
    "\n",
    "    print(\"Training complete. Best val acc:\", best_val_acc)\n",
    "\n",
    "    # Save plots and confusion matrix\n",
    "    plot_metrics(train_losses, val_losses, train_accs, val_accs, OUT_DIR)\n",
    "    compute_and_save_confusion(model, val_loader, device, class_names, OUT_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd735d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MOSHINPOWER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
