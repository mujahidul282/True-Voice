{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2d9044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from typing import Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e64b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TRAIN_DIR = r\"C:\\Users\\sudeepta\\Desktop\\DR_project\\augmented_resized_V2\\train\"\n",
    "DEFAULT_VAL_DIR   = r\"C:\\Users\\sudeepta\\Desktop\\DR_project\\augmented_resized_V2\\val\"\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 16        # reduce if OOM (8 or 4)\n",
    "PROJ_DIM = 256\n",
    "HEAD_EPOCHS = 20\n",
    "PARTIAL_EPOCHS = 0\n",
    "FINE_EPOCHS = 30\n",
    "LR = 1e-3\n",
    "OUT_DIR = \"outputs_resnet_vision_fusion\"\n",
    "CHECKPOINT = \"hybrid_resnet_vision.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bbbe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel=3, stride=1, padding=1, groups=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel, stride, padding, groups=groups, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "class VisionNet(nn.Module):\n",
    "    \"\"\"Small example VisionNet\"\"\"\n",
    "    def __init__(self, in_channels=3, out_features=512):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            ConvBlock(in_channels, 32, stride=2),\n",
    "            ConvBlock(32, 32),\n",
    "            ConvBlock(32, 64, stride=2),\n",
    "        )\n",
    "        self.stage1 = nn.Sequential(\n",
    "            ConvBlock(64, 128),\n",
    "            ConvBlock(128, 128),\n",
    "        )\n",
    "        self.stage2 = nn.Sequential(\n",
    "            ConvBlock(128, 256, stride=2),\n",
    "            ConvBlock(256, 256),\n",
    "        )\n",
    "        self.stage3 = nn.Sequential(\n",
    "            ConvBlock(256, 512, stride=2),\n",
    "            ConvBlock(512, 512),\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self._out = out_features\n",
    "        self.project = nn.Linear(512, self._out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.pool(x).flatten(1)\n",
    "        x = self.project(x)\n",
    "        return x\n",
    "\n",
    "    def feature_dim(self):\n",
    "        return self._out\n",
    "\n",
    "class VisionMamba(nn.Module):\n",
    "    \"\"\"Small example VisionMamba using separable/depthwise style ops\"\"\"\n",
    "    def __init__(self, in_channels=3, out_features=512):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            ConvBlock(in_channels, 24, stride=2),\n",
    "            ConvBlock(24, 24),\n",
    "        )\n",
    "        self.depthwise = nn.Sequential(\n",
    "            ConvBlock(24, 48, groups=24, stride=2),\n",
    "            ConvBlock(48, 48, groups=48),\n",
    "            ConvBlock(48, 96, stride=2),\n",
    "            ConvBlock(96, 96),\n",
    "        )\n",
    "        self.spp = nn.Sequential(\n",
    "            ConvBlock(96, 192, kernel=1, padding=0),\n",
    "            ConvBlock(192, 192),\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self._out = out_features\n",
    "        self.project = nn.Linear(192, self._out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.depthwise(x)\n",
    "        x = self.spp(x)\n",
    "        x = self.pool(x).flatten(1)\n",
    "        x = self.project(x)\n",
    "        return x\n",
    "\n",
    "    def feature_dim(self):\n",
    "        return self._out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f67be92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50_Feature(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        net = models.resnet50(pretrained=pretrained)\n",
    "        # chop off final fc\n",
    "        self.encoder = nn.Sequential(*list(net.children())[:-1])  # ends with AdaptiveAvgPool2d\n",
    "        self.out_dim = 2048\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x).flatten(1)\n",
    "        return x\n",
    "\n",
    "    def feature_dim(self):\n",
    "        return self.out_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac63045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedProjection(nn.Module):\n",
    "    def __init__(self, in_dim, proj_dim=PROJ_DIM):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, proj_dim)\n",
    "        self.bn = nn.BatchNorm1d(proj_dim)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(proj_dim, max(8, proj_dim // 8)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(max(8, proj_dim // 8), 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.act(self.bn(self.fc(x)))\n",
    "        g = self.gate(z)   # (B,1)\n",
    "        return z * g\n",
    "\n",
    "class HybridResNetVisionFusion(nn.Module):\n",
    "    def __init__(self, proj_dim=PROJ_DIM, num_classes=5, pretrained_resnet=True):\n",
    "        super().__init__()\n",
    "        self.resnet = ResNet50_Feature(pretrained=pretrained_resnet)\n",
    "        self.visionnet = VisionNet(out_features=proj_dim)      # output proj_dim from project layer\n",
    "        self.visionmamba = VisionMamba(out_features=proj_dim)  # same\n",
    "\n",
    "        # For resnet, map 2048 -> proj_dim via gated proj\n",
    "        self.proj_r = GatedProjection(self.resnet.feature_dim(), proj_dim)\n",
    "        # visionnet and visionmamba already output proj_dim, but we'll still apply gating to keep interface uniform\n",
    "        self.proj_vn = GatedProjection(self.visionnet.feature_dim(), proj_dim)\n",
    "        self.proj_vm = GatedProjection(self.visionmamba.feature_dim(), proj_dim)\n",
    "\n",
    "        fused_dim = proj_dim * 3\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        fr = self.resnet(x)        # (B,2048)\n",
    "        fvn = self.visionnet(x)    # (B,proj_dim)\n",
    "        fvm = self.visionmamba(x)  # (B,proj_dim)\n",
    "        pr = self.proj_r(fr)\n",
    "        pvn = self.proj_vn(fvn)\n",
    "        pvm = self.proj_vm(fvm)\n",
    "        fused = torch.cat([pr, pvn, pvm], dim=1)\n",
    "        out = self.head(fused)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08deb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(train_dir: str, val_dir: str, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, num_workers=4) -> Tuple[DataLoader, DataLoader]:\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(image_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(0.15,0.15,0.15,0.02),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize(int(image_size*1.14)),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    train_ds = datasets.ImageFolder(train_dir, transform=train_tf)\n",
    "    val_ds = datasets.ImageFolder(val_dir, transform=val_tf)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3994e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device); labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        total += imgs.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device); labels = labels.to(device)\n",
    "            logits = model(imgs)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "            total += imgs.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    return running_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee39796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(train_losses, val_losses, train_accs, val_accs, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    epochs = range(1, len(train_losses)+1)\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_losses, marker='o'); plt.plot(epochs, val_losses, marker='o')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Training & Validation Loss'); plt.legend(['train','val']); plt.grid(True)\n",
    "    plt.savefig(os.path.join(out_dir, 'loss_curve.png')); plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_accs, marker='o'); plt.plot(epochs, val_accs, marker='o')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Training & Validation Accuracy'); plt.legend(['train','val']); plt.grid(True)\n",
    "    plt.savefig(os.path.join(out_dir, 'acc_curve.png')); plt.close()\n",
    "    print(\"Saved loss/accuracy plots to\", out_dir)\n",
    "\n",
    "def confusion_and_report(model, loader, device, class_names: List[str], out_dir: str):\n",
    "    model.eval()\n",
    "    y_true = []; y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            logits = model(imgs)\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            y_pred.extend(preds.tolist())\n",
    "            y_true.extend(labels.numpy().tolist())\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion matrix:\\n\", cm)\n",
    "    print(\"Classification report:\\n\", classification_report(y_true, y_pred, target_names=class_names))\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    ticks = np.arange(len(class_names))\n",
    "    plt.xticks(ticks, class_names, rotation=45); plt.yticks(ticks, class_names)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(out_dir, 'confusion_matrix.png')); plt.close()\n",
    "    print(\"Saved confusion matrix to\", out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a6005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train-dir', type=str, default=DEFAULT_TRAIN_DIR)\n",
    "    parser.add_argument('--val-dir', type=str, default=DEFAULT_VAL_DIR)\n",
    "    parser.add_argument('--batch', type=int, default=BATCH_SIZE)\n",
    "    parser.add_argument('--proj-dim', type=int, default=PROJ_DIM)\n",
    "    parser.add_argument('--head-epochs', type=int, default=HEAD_EPOCHS)\n",
    "    parser.add_argument('--partial-epochs', type=int, default=PARTIAL_EPOCHS)\n",
    "    parser.add_argument('--fine-epochs', type=int, default=FINE_EPOCHS)\n",
    "    parser.add_argument('--lr', type=float, default=LR)\n",
    "    parser.add_argument('--out-dir', type=str, default=OUT_DIR)\n",
    "    parser.add_argument('--checkpoint', type=str, default=CHECKPOINT)\n",
    "    args, _ = parser.parse_known_args()  # safe in notebooks\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    train_loader, val_loader = get_loaders(args.train_dir, args.val_dir, image_size=IMAGE_SIZE, batch_size=args.batch)\n",
    "    class_names = sorted([d for d in os.listdir(args.train_dir) if os.path.isdir(os.path.join(args.train_dir, d))])\n",
    "    print(\"Classes:\", class_names)\n",
    "\n",
    "    # Build model (visionnet/vm already output proj-dim from project, but we still use gated proj for consistency)\n",
    "    model = HybridResNetVisionFusion(proj_dim=args.proj_dim, num_classes=len(class_names), pretrained_resnet=True)\n",
    "    model.to(device)\n",
    "\n",
    "    # Stage 1: freeze backbones (resnet, visionnet, visionmamba) except projection heads + head\n",
    "    for p in model.resnet.parameters(): p.requires_grad = False\n",
    "    for p in model.visionnet.parameters(): p.requires_grad = False\n",
    "    for p in model.visionmamba.parameters(): p.requires_grad = False\n",
    "    for p in model.proj_r.parameters(): p.requires_grad = True\n",
    "    for p in model.proj_vn.parameters(): p.requires_grad = True\n",
    "    for p in model.proj_vm.parameters(): p.requires_grad = True\n",
    "    for p in model.head.parameters(): p.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, weight_decay=1e-4)\n",
    "\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    print(f\"Head-only training for {args.head_epochs} epochs...\")\n",
    "    for epoch in range(1, args.head_epochs + 1):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, device)\n",
    "        train_losses.append(tr_loss); val_losses.append(val_loss)\n",
    "        train_accs.append(tr_acc); val_accs.append(val_acc)\n",
    "        print(f\"[Head] Epoch {epoch}: train_loss={tr_loss:.4f}, train_acc={tr_acc:.4f} | val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({'epoch': epoch, 'model_state': model.state_dict(), 'val_acc': val_acc}, args.checkpoint)\n",
    "\n",
    "    # Partial unfreeze (if requested)\n",
    "    if args.partial_epochs > 0:\n",
    "        print(f\"Partial unfreeze for {args.partial_epochs} epochs...\")\n",
    "        # Example unfreeze â€” adapt as desired\n",
    "        for p in list(model.visionnet.stage3.parameters()):\n",
    "            p.requires_grad = True\n",
    "        for p in list(model.visionmamba.spp.parameters()):\n",
    "            p.requires_grad = True\n",
    "        optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr * 0.1, weight_decay=1e-4)\n",
    "        for epoch in range(1, args.partial_epochs + 1):\n",
    "            tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, device)\n",
    "            val_loss, val_acc = evaluate(model, val_loader, device)\n",
    "            train_losses.append(tr_loss); val_losses.append(val_loss)\n",
    "            train_accs.append(tr_acc); val_accs.append(val_acc)\n",
    "            print(f\"[Partial] Epoch {epoch}: val_acc={val_acc:.4f}\")\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save({'epoch': epoch, 'model_state': model.state_dict(), 'val_acc': val_acc}, args.checkpoint)\n",
    "\n",
    "    # Full fine-tune\n",
    "    print(f\"Full fine-tune for {args.fine_epochs} epochs...\")\n",
    "    for p in model.parameters(): p.requires_grad = True\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr * 0.01, weight_decay=1e-5)\n",
    "    for i in range(1, args.fine_epochs + 1):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, device)\n",
    "        train_losses.append(tr_loss); val_losses.append(val_loss)\n",
    "        train_accs.append(tr_acc); val_accs.append(val_acc)\n",
    "        print(f\"[Fine] Epoch {i}: train_loss={tr_loss:.4f}, train_acc={tr_acc:.4f} | val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({'epoch': i, 'model_state': model.state_dict(), 'val_acc': val_acc}, args.checkpoint)\n",
    "\n",
    "    print(\"Training finished. Best val acc:\", best_val_acc)\n",
    "    plot_metrics(train_losses, val_losses, train_accs, val_accs, args.out_dir)\n",
    "    confusion_and_report(model, val_loader, device, class_names, args.out_dir)\n",
    "    print(\"Outputs saved to\", args.out_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a78fae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
